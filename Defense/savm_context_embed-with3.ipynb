{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e023954-e226-4c20-abca-41a6c5b24d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os, json, glob, hashlib, math, gc\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Iterable, Tuple\n",
    "\n",
    "# Data / math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# ML\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# HF\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel  # base model to get hidden_states\n",
    "\n",
    "# Style\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"axes.spines.top\": False, \"axes.spines.right\": False,\n",
    "    \"axes.grid\": True, \"grid.alpha\": 0.22,\n",
    "    \"axes.titleweight\": \"bold\", \"axes.titlesize\": 13,\n",
    "    \"axes.labelsize\": 12, \"legend.frameon\": False, \"font.size\": 11,\n",
    "})\n",
    "\n",
    "RNG = np.random.RandomState(42)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "HF_MODEL_ID = \"Qwen/Qwen3-8B\"   # use your exact HF id here\n",
    "\n",
    "# === Embedding layer aggregation config ===\n",
    "EMB_MODE = \"mid_k\"          # options: \"last_k\" | \"mid_k\" | \"layer_ids\"\n",
    "\n",
    "LAST_K = 4                  # used if EMB_MODE == \"last_k\"\n",
    "MID_K = 6                   # how many middle layers to average\n",
    "MID_CENTER_FRAC = 0.50      # center of the window as a fraction in [0,1], 0.5 = exact middle\n",
    "\n",
    "LAYER_IDS = [10,11,12]      # used if EMB_MODE == \"layer_ids\" (zero-based model layers)\n",
    "                            # NOTE: hidden_states[0] is the embedding table output,\n",
    "                            #       model layers are hidden_states[1..N], so layer i -> hs[i+1]\n",
    "\n",
    "LAYER_AVG_LAST_K = 4             # average last-k hidden layers\n",
    "MAX_TOK_LEN = 2048               # truncate if needed\n",
    "BATCH_EMB = 4                    # small batch to avoid OOM with 14B\n",
    "SAVE_DIR = \"./artifacts_svm\"     # where we’ll save embeddings & csv\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e63fd0-b65c-493d-bf3e-2fe9c599f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _aggregate_token_layers(hs, emb_mode=\"mid_k\",\n",
    "                            last_k=4, mid_k=6, mid_center_frac=0.5, layer_ids=None):\n",
    "    \"\"\"\n",
    "    hs: tuple of hidden_states from HF (len = n_layers + 1; hs[0] = embeddings)\n",
    "    returns: token_emb [B,T,H], used layers (zero-based model layer indices)\n",
    "    \"\"\"\n",
    "    n_layers = len(hs) - 1                 # exclude hs[0] (input embeddings)\n",
    "    if n_layers <= 0:\n",
    "        raise ValueError(\"No model layers found in hidden_states\")\n",
    "\n",
    "    if emb_mode == \"last_k\":\n",
    "        k = max(1, min(last_k, n_layers))\n",
    "        idx_model = list(range(n_layers - k, n_layers))             # 0-based in [0..n_layers-1]\n",
    "    elif emb_mode == \"mid_k\":\n",
    "        k = max(1, min(mid_k, n_layers))\n",
    "        center = int(round(mid_center_frac * (n_layers - 1)))       # 0..n_layers-1\n",
    "        start = max(0, center - k // 2)\n",
    "        end   = min(n_layers, start + k)\n",
    "        idx_model = list(range(start, end))\n",
    "    elif emb_mode == \"layer_ids\":\n",
    "        ids = layer_ids or []\n",
    "        idx_model = [i for i in ids if 0 <= i < n_layers]\n",
    "        if len(idx_model) == 0:\n",
    "            raise ValueError(\"LAYER_IDS produced an empty selection\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown emb_mode: {emb_mode}\")\n",
    "\n",
    "    # Map model-layer index -> hidden_states index (+1)\n",
    "    tensors = [hs[i+1] for i in idx_model]                          # each [B,T,H]\n",
    "    token_emb = torch.stack(tensors, 0).mean(0) if len(tensors) > 1 else tensors[0]\n",
    "    return token_emb, idx_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cf5bf-bcc1-4614-a2bf-121c215598af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Loaded:\", HF_MODEL_ID, \"on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47806e2a-553f-41ea-b00b-9dccebac8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_paths(paths_or_globs: List[str]) -> List[str]:\n",
    "    out=[]\n",
    "    for p in paths_or_globs:\n",
    "        if any(ch in p for ch in \"*?[]\"):\n",
    "            out.extend(glob.glob(p, recursive=True))\n",
    "        else:\n",
    "            out.append(p)\n",
    "    return [p for p in out if Path(p).exists()]\n",
    "\n",
    "def _iter_behavior_like(obj: Any):\n",
    "    # matches your structures where behaviors / strategies exist\n",
    "    if isinstance(obj, dict):\n",
    "        if isinstance(obj.get(\"behaviors\"), dict):\n",
    "            for v in obj[\"behaviors\"].values():\n",
    "                if isinstance(v, dict):\n",
    "                    yield v\n",
    "        # fallthrough scan\n",
    "        for v in obj.values():\n",
    "            yield from _iter_behavior_like(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            yield from _iter_behavior_like(v)\n",
    "\n",
    "def _rows_from_holder(holder: Dict[str, Any], behavior_number=None, behavior_text=None, source_path:str=\"\"):\n",
    "    set_number = holder.get(\"set_number\")\n",
    "    strategy_number = holder.get(\"strategy_number\")\n",
    "    conv = holder.get(\"conversation\", []) or []\n",
    "    for turn in conv:\n",
    "        # many files have either \"turn\" starting at 1 or implicit order\n",
    "        t_idx = int(turn.get(\"turn\", 0))  # we’ll normalize to 0-based later\n",
    "        eva = turn.get(\"evaluation\") if isinstance(turn.get(\"evaluation\"), dict) else {}\n",
    "        score = turn.get(\"evaluation_score\") or turn.get(\"eval_score\") or eva.get(\"score\")\n",
    "        reason = turn.get(\"evaluation_reason\") or eva.get(\"reason\")\n",
    "        if score is None:\n",
    "            continue\n",
    "        attacker = (turn.get(\"attacker\") or \"\").strip()\n",
    "        target   = (turn.get(\"target\")   or \"\").strip()\n",
    "        row = {\n",
    "            \"source_path\": source_path,\n",
    "            \"behavior_number\": behavior_number,\n",
    "            \"behavior_text\": behavior_text,\n",
    "            \"set_number\": set_number,\n",
    "            \"strategy_number\": strategy_number,\n",
    "            \"turn_idx_raw\": t_idx,                 # may be 1-based in file\n",
    "            \"score\": int(score),\n",
    "            \"reason\": reason,\n",
    "            \"attacker\": attacker,\n",
    "            \"target\": target,\n",
    "            \"context_text\": f\"Attacker: {attacker}\\nTarget: {target}\".strip(),\n",
    "        }\n",
    "        yield row\n",
    "\n",
    "def load_scored_contexts(json_inputs: List[str]) -> pd.DataFrame:\n",
    "    rows=[]\n",
    "    for p in _expand_paths(json_inputs):\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        behs = list(_iter_behavior_like(data)) or [data]\n",
    "        for beh in behs:\n",
    "            bnum = beh.get(\"behavior_number\")\n",
    "            btxt = (beh.get(\"behavior\", {}) or {}).get(\"Behavior\") if isinstance(beh.get(\"behavior\"), dict) else None\n",
    "            # either strategies list, or the behavior directly contains conversation nodes\n",
    "            if isinstance(beh.get(\"strategies\"), list) and beh[\"strategies\"]:\n",
    "                for strat in beh[\"strategies\"]:\n",
    "                    for r in _rows_from_holder(strat, bnum, btxt, p):\n",
    "                        rows.append(r)\n",
    "            else:\n",
    "                # treat beh as “holder”\n",
    "                for r in _rows_from_holder(beh, bnum, btxt, p):\n",
    "                    rows.append(r)\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"Loaded 0 rows.\")\n",
    "        return df\n",
    "\n",
    "    # Group id = file+behavior+set+strategy to keep a single conversation thread\n",
    "    def _gid(row):\n",
    "        key = f\"{row.get('source_path','')}|{row.get('behavior_number','')}|{row.get('set_number','')}|{row.get('strategy_number','')}\"\n",
    "        return hashlib.md5(key.encode()).hexdigest()\n",
    "    df[\"group_id\"] = df.apply(_gid, axis=1)\n",
    "\n",
    "    # Normalize a consistent 0-based turn within each group in file order\n",
    "    df = (df.sort_values([\"group_id\",\"turn_idx_raw\"])\n",
    "            .groupby(\"group_id\", as_index=False)\n",
    "            .apply(lambda g: g.assign(turn_local0=np.arange(len(g)),\n",
    "                                      turn_local1=np.arange(1,len(g)+1)))\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # final label buckets you use elsewhere\n",
    "    df[\"label\"] = np.select(\n",
    "        [\n",
    "            df[\"score\"].isin([1,2]),\n",
    "            df[\"score\"].isin([4,5])\n",
    "        ],\n",
    "        [0, 1],  # 0=safe, 1=unsafe\n",
    "        default=None\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6c217-20ca-4dd1-85ce-e8be1ad9b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build messages list for a given group up to (and including) current attacker,\n",
    "#    but EXCLUDING the assistant reply of that turn (so: “before it answers”).\n",
    "def messages_for_turn(g: pd.DataFrame, idx_local0: int, system_text:str=None) -> List[Dict[str,str]]:\n",
    "    msgs=[]\n",
    "    if system_text:\n",
    "        msgs.append({\"role\":\"system\",\"content\":system_text})\n",
    "\n",
    "    # all previous completed turns: add as user/assistant pairs\n",
    "    # note: your data has attacker (user) and target (assistant).\n",
    "    for i in range(idx_local0):\n",
    "        a = (g.iloc[i][\"attacker\"] or \"\").strip()\n",
    "        t = (g.iloc[i][\"target\"] or \"\").strip()\n",
    "        if a:\n",
    "            msgs.append({\"role\":\"user\",\"content\":a})\n",
    "        if t:\n",
    "            msgs.append({\"role\":\"assistant\",\"content\":t})\n",
    "\n",
    "    # add current user prompt (attacker) only\n",
    "    cur_a = (g.iloc[idx_local0][\"attacker\"] or \"\").strip()\n",
    "    if cur_a:\n",
    "        msgs.append({\"role\":\"user\",\"content\":cur_a})\n",
    "\n",
    "    return msgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_rendered_prompts(rendered_input_ids: torch.Tensor, attention_mask: torch.Tensor) -> np.ndarray:\n",
    "    # Forward with hidden states\n",
    "    out = model(input_ids=rendered_input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    hs = out.hidden_states  # tuple[layer] shape: (B, T, H)\n",
    "    # # average last-k layers\n",
    "    # if LAYER_AVG_LAST_K > 1:\n",
    "    #     token_emb = torch.stack(hs[-LAYER_AVG_LAST_K:], 0).mean(0)  # (B,T,H)\n",
    "    # else:\n",
    "    #     token_emb = hs[-1]\n",
    "    # # mean-pool over valid tokens\n",
    "    # mask = attention_mask.unsqueeze(-1)  # (B,T,1)\n",
    "    # sent = (token_emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n",
    "    # # DO NOT L2-normalize here since you asked for “without normalization” in some analyses.\n",
    "   \n",
    "    # NEW: select middle layers (or whatever EMB_MODE says)\n",
    "    token_emb, used_layers = _aggregate_token_layers(\n",
    "        hs,\n",
    "        emb_mode=EMB_MODE,\n",
    "        last_k=LAST_K,\n",
    "        mid_k=MID_K,\n",
    "        mid_center_frac=MID_CENTER_FRAC,\n",
    "        layer_ids=LAYER_IDS,\n",
    "    )\n",
    "    # mean-pool over tokens with mask (no L2-normalization unless you want it)\n",
    "    mask = attention_mask.unsqueeze(-1)\n",
    "    sent = (token_emb * mask).sum(1) / mask.sum(1).clamp(min=1)\n",
    "    emb = sent.float().cpu().numpy()\n",
    "    \n",
    "    return emb\n",
    "\n",
    "def render_and_embed_messages_batch(batch_msgs: List[List[Dict[str,str]]]) -> np.ndarray:\n",
    "    # use chat template exactly as runtime\n",
    "    batch_inputs = tokenizer.apply_chat_template(\n",
    "        batch_msgs,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOK_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = batch_inputs.to(device)\n",
    "    attn = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return embed_rendered_prompts(input_ids, attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890e509-f71a-4681-8f3b-d69931ac2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_turn_context_embeddings(\n",
    "    df: pd.DataFrame,\n",
    "    system_text: str = None,\n",
    "    allowed_scores: set = {1, 2,3, 4, 5},   # ⬅️ keep 3 in history, exclude from outputs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each turn in df, build the *runtime* pre-answer context:\n",
    "      [system] + all (user,assistant) pairs up to t-1 + current user(t).\n",
    "    Embed/return ONLY rows whose score ∈ allowed_scores.\n",
    "    History can include any scores (including 3).\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "\n",
    "    for gid, g in df.groupby(\"group_id\", sort=False):\n",
    "        g = g.sort_values(\"turn_local0\").reset_index(drop=True)\n",
    "\n",
    "        pending_msgs, pending_idxs = [], []\n",
    "        for i in range(len(g)):\n",
    "            msgs = messages_for_turn(g, i, system_text=system_text)\n",
    "\n",
    "            # enqueue only if this turn’s score is allowed\n",
    "            if (allowed_scores is None) or (g.at[i, \"score\"] in allowed_scores):\n",
    "                pending_msgs.append(msgs)\n",
    "                pending_idxs.append(i)\n",
    "\n",
    "            # flush batch if full or at end\n",
    "            if pending_msgs and (len(pending_msgs) == BATCH_EMB or i == len(g) - 1):\n",
    "                vecs = render_and_embed_messages_batch(pending_msgs)  # (B,H)\n",
    "                for j, vec in zip(pending_idxs, vecs):\n",
    "                    row = g.iloc[j].to_dict()\n",
    "                    row[\"emb\"] = vec\n",
    "                    out_rows.append(row)\n",
    "                pending_msgs.clear()\n",
    "                pending_idxs.clear()\n",
    "\n",
    "    df_emb = pd.DataFrame(out_rows)\n",
    "    if df_emb.empty:\n",
    "        print(\"[warn] No rows embedded (did your data have only score==3?)\")\n",
    "        return df_emb\n",
    "\n",
    "    E = np.stack(df_emb[\"emb\"].to_numpy(), axis=0)\n",
    "    np.save(Path(SAVE_DIR) / \"turn_context_embeddings.npy\", E)\n",
    "    meta_cols = [c for c in df_emb.columns if c != \"emb\"]\n",
    "    df_emb[meta_cols].to_parquet(Path(SAVE_DIR) / \"turn_context_meta.parquet\", index=False)\n",
    "    print(f\"Saved: {E.shape} → {Path(SAVE_DIR) / 'turn_context_embeddings.npy'}\")\n",
    "    print(f\"Saved meta → {Path(SAVE_DIR) / 'turn_context_meta.parquet'}\")\n",
    "    return df_emb\n",
    "\n",
    "\n",
    "# ---- Load your files and compute embeddings ----\n",
    "JSON_INPUTS = [\n",
    "    \"/storage/users/visionintelligence/Nivya/x-teaming/attacks/2025-08-14_01-31-41_HINDI_50engStrategy/all_results.json\",\n",
    "    \"/storage/users/visionintelligence/Nivya/x-teaming/attacks/2025-08-13_13-43-15_FRENCH_50engStrategy/all_results.json\",\n",
    "    \"/storage/users/visionintelligence/Nivya/x-teaming/attacks/2025-08-15_02-58-29/all_results.json\",\n",
    "    \"/storage/users/visionintelligence/Nivya/x-teaming/attacks/2025-08-22_10-54-11/all_results.json\",\n",
    "    \"/storage/users/visionintelligence/Nivya/x-teaming/attacks/Hindi_attackThinkTrue_2025-08-27_06-46-57/all_results.json\",\n",
    "    \"/storage/users/visionintelligence/Nivya/x-teaming/attacks/French_attackThinkTrue_2025-08-27_02-24-42/all_results.json\"\n",
    "    # e.g., your Hindi_50engStrategy files, etc.\n",
    "    # \"/another/path/all_results.json\",\n",
    "]\n",
    "df_all = load_scored_contexts(JSON_INPUTS)\n",
    "print(\"Loaded rows:\", len(df_all))\n",
    "\n",
    "# 1) Identify those rows (you already printed them)\n",
    "suspect = df_all[(df_all[\"turn_local0\"]==0) & (df_all[\"attacker\"].fillna(\"\").str.strip()==\"\")]\n",
    "bad_gids = suspect[\"group_id\"].unique().tolist()\n",
    "print(\"Fixing groups:\", bad_gids)\n",
    "\n",
    "df_all = df_all[df_all[\"score\"].isin([1, 2, 3, 4, 5])].copy()\n",
    "# 2) Drop the offending first turn(s)\n",
    "df_all = df_all[~((df_all[\"group_id\"].isin(bad_gids)) & (df_all[\"turn_local0\"]==0))].copy()\n",
    "\n",
    "# 3) Recompute 0-based / 1-based local indices so trajectories start at 0 again\n",
    "df_all = (df_all.sort_values([\"group_id\",\"turn_local0\"])\n",
    "                 .groupby(\"group_id\", as_index=False)\n",
    "                 .apply(lambda g: g.assign(\n",
    "                     turn_local0=np.arange(len(g)),\n",
    "                     turn_local1=np.arange(1, len(g)+1)\n",
    "                 ))\n",
    "                 .reset_index(drop=True))\n",
    "\n",
    "print(\"After drop+reindex:\", len(df_all))\n",
    "\n",
    "df_emb = compute_turn_context_embeddings(df_all,allowed_scores={1,2,3,4,5}, system_text=None)  # add system text if you use one at runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262e46f-4f59-4e6b-b954-56186f42f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep scores 1,2,3,4,5 but use label: 0 for {1,2}, 1 for {4,5}; 3 is “neutral/other”, keep for analysis but not for SVM.\n",
    "df_train = df_emb.copy()\n",
    "# df_train[\"label\"] = df_train[\"score\"].isin([4,5]).astype(int)  # 1=unsafe(4/5), 0=safe(1/2); 3 kept for analysis\n",
    "\n",
    "\n",
    "# Optional: drop score==3 from SVM training but keep them for correlation plots if you want\n",
    "df_svm = df_train[df_train[\"score\"].isin([1,2,4,5])].copy()\n",
    "\n",
    "# Balance (downsample the majority) IF you want\n",
    "def downsample_to_balance(df_lbl: pd.DataFrame) -> pd.DataFrame:\n",
    "    g0 = df_lbl[df_lbl[\"label\"]==0]\n",
    "    g1 = df_lbl[df_lbl[\"label\"]==1]\n",
    "    if len(g0)==0 or len(g1)==0:\n",
    "        return df_lbl\n",
    "    n = min(len(g0), len(g1))\n",
    "    g0s = resample(g0, replace=False, n_samples=n, random_state=42)\n",
    "    g1s = resample(g1, replace=False, n_samples=n, random_state=42)\n",
    "    return pd.concat([g0s,g1s], ignore_index=True)\n",
    "\n",
    "USE_DOWNSAMPLE = True\n",
    "df_svm_bal = downsample_to_balance(df_svm) if USE_DOWNSAMPLE else df_svm\n",
    "\n",
    "# Build X, y\n",
    "X = np.stack(df_svm_bal[\"emb\"].to_numpy(), axis=0)  # no normalization, per your request\n",
    "y = df_svm_bal[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "print(\"SVM train shape:\", X.shape, \"positives:\", (y==1).sum(), \"negatives:\", (y==0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03875b42-4c8b-4dc5-99d9-0824c47bfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(C=1.0, class_weight=\"balanced\", random_state=42)\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Add margins back to the full turn table for plotting (use the model on all rows)\n",
    "X_all = np.stack(df_train[\"emb\"].to_numpy(), axis=0)\n",
    "margins_all = svm.decision_function(X_all)  # signed distance (up to scale)\n",
    "df_plot = df_train.copy()\n",
    "df_plot[\"margin\"] = margins_all\n",
    "\n",
    "# Save everything for reuse\n",
    "np.save(Path(SAVE_DIR)/\"svm_w.npy\", svm.coef_.astype(np.float32))\n",
    "np.save(Path(SAVE_DIR)/\"svm_b.npy\", np.array([svm.intercept_[0]], dtype=np.float32))\n",
    "df_plot.to_parquet(Path(SAVE_DIR)/\"df_plot_with_margins.parquet\", index=False)\n",
    "\n",
    "print(\"Saved SVM & df_plot with margins into\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d39e5-0f5d-48ef-9500-5fbf3a324a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on the training set (since you asked “use all samples”)\n",
    "pred_all = (margins_all > 0).astype(int)\n",
    "\n",
    "# Only evaluate classification on 1/2 vs 4/5 rows\n",
    "mask_svm_eval = df_plot[\"score\"].isin([1,2,4,5])\n",
    "print(\"Accuracy:\", (pred_all[mask_svm_eval]==df_plot.loc[mask_svm_eval,\"label\"].to_numpy()).mean())\n",
    "try:\n",
    "    print(\"ROC-AUC:\", roc_auc_score(df_plot.loc[mask_svm_eval,\"label\"].to_numpy(),\n",
    "                                    df_plot.loc[mask_svm_eval,\"margin\"].to_numpy()))\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC error:\", e)\n",
    "\n",
    "print(\"\\nClassification report:\\n\",\n",
    "      classification_report(df_plot.loc[mask_svm_eval,\"label\"].to_numpy(),\n",
    "                            pred_all[mask_svm_eval], digits=3))\n",
    "\n",
    "# === Turn-level correlation: margin vs score (1..5) ===\n",
    "pear = pearsonr(df_plot[\"margin\"], df_plot[\"score\"])\n",
    "spear = spearmanr(df_plot[\"margin\"], df_plot[\"score\"])\n",
    "print(f\"\\nPearson(margin, score): {pear.statistic:.3f} (p={pear.pvalue:.2g})\")\n",
    "print(f\"Spearman(margin, score): {spear.correlation:.3f} (p={spear.pvalue:.2g})\")\n",
    "\n",
    "# (Optional) final-turn-only correlation\n",
    "final_rows = (df_plot.sort_values([\"group_id\",\"turn_local0\"])\n",
    "                    .groupby(\"group_id\", as_index=False).tail(1))\n",
    "pear_f = pearsonr(final_rows[\"margin\"], final_rows[\"score\"])\n",
    "spear_f = spearmanr(final_rows[\"margin\"], final_rows[\"score\"])\n",
    "print(f\"\\n[Final turn] Pearson: {pear_f.statistic:.3f} (p={pear_f.pvalue:.2g})\")\n",
    "print(f\"[Final turn] Spearman: {spear_f.correlation:.3f} (p={spear_f.pvalue:.2g})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a87ee-d759-4dc3-89fa-7ee9fbc12792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell I — Core visualizations from df_plot =====\n",
    "TAU_EARLY = 0.20\n",
    "TAU_BLOCK = 0.40\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "assert {\"group_id\",\"turn_local0\",\"score\",\"label\",\"margin\"}.issubset(df_plot.columns), \\\n",
    "    \"df_plot must have: group_id, turn_local0, score, label, margin\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1) Histogram: margins by class (safe vs unsafe)\n",
    "# ---------------------------------------------\n",
    "mask_eval = df_plot[\"score\"].isin([1,2,4,5])  # strict eval on 1/2 vs 4/5\n",
    "safe   = df_plot.loc[(df_plot[\"label\"]==0) & mask_eval, \"margin\"].to_numpy()\n",
    "unsafe = df_plot.loc[(df_plot[\"label\"]==1) & mask_eval, \"margin\"].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(6.6,4.2))\n",
    "plt.hist(safe,   bins=40, alpha=0.65, label=\"safe (1/2)\",   density=True)\n",
    "plt.hist(unsafe, bins=40, alpha=0.65, label=\"unsafe (4/5)\", density=True)\n",
    "plt.axvline(0.0,        ls=\"--\", lw=1, color=\"k\", label=\"τ=0\")\n",
    "plt.axvline(TAU_EARLY,  ls=\"--\", lw=1, label=f\"τ_early={TAU_EARLY:.2f}\")\n",
    "plt.axvline(TAU_BLOCK,  ls=\"--\", lw=1, label=f\"τ_block={TAU_BLOCK:.2f}\")\n",
    "plt.title(\"Margin distribution (pre-answer, chat-templated)\")\n",
    "plt.xlabel(\"SVM margin (w·x + b)\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) ROC & PR curves\n",
    "# ---------------------------------------------\n",
    "y_true = df_plot.loc[mask_eval, \"label\"].to_numpy().astype(int)\n",
    "scores = df_plot.loc[mask_eval, \"margin\"].to_numpy()\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_true, scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "prec, rec, thr_pr = precision_recall_curve(y_true, scores)\n",
    "ap = average_precision_score(y_true, scores)\n",
    "\n",
    "plt.figure(figsize=(6.2,4.2))\n",
    "plt.plot(fpr, tpr, lw=2)\n",
    "plt.title(f\"ROC (AUC={roc_auc:.3f})\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid(alpha=.25); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6.2,4.2))\n",
    "plt.plot(rec, prec, lw=2)\n",
    "plt.title(f\"Precision–Recall (AP={ap:.3f})\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.grid(alpha=.25); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3) Threshold sweep: Precision / Recall / F1 vs τ\n",
    "# ---------------------------------------------\n",
    "taus = np.unique(np.percentile(scores, np.linspace(1, 99, 60)))\n",
    "P,R,F = [],[],[]\n",
    "for t in taus:\n",
    "    yhat = (scores >= t).astype(int)\n",
    "    p_, r_, f_, _ = precision_recall_fscore_support(y_true, yhat, average=\"binary\", zero_division=0)\n",
    "    P.append(p_); R.append(r_); F.append(f_)\n",
    "\n",
    "plt.figure(figsize=(7.6,4.6))\n",
    "plt.plot(taus, P, label=\"Precision\", lw=2)\n",
    "plt.plot(taus, R, label=\"Recall\",    lw=2)\n",
    "plt.plot(taus, F, label=\"F1\",        lw=2)\n",
    "for v,lab in [(0.0,\"τ=0\"), (TAU_EARLY,f\"τ_early={TAU_EARLY:.2f}\"), (TAU_BLOCK,f\"τ_block={TAU_BLOCK:.2f}\")]:\n",
    "    plt.axvline(v, ls=\"--\", lw=1, label=lab)\n",
    "plt.title(\"Precision / Recall / F1 vs τ\")\n",
    "plt.xlabel(\"τ (threshold on margin)\"); plt.ylabel(\"score\")\n",
    "plt.ylim(0,1.02); plt.legend(); plt.grid(alpha=.25); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4) Confusion matrix helper and readouts at key τ's\n",
    "# ---------------------------------------------\n",
    "def cm_at_tau(scores, labels, tau):\n",
    "    yhat = (scores >= tau).astype(int)\n",
    "    cm = confusion_matrix(labels, yhat, labels=[0,1])  # [[TN,FP],[FN,TP]]\n",
    "    (tn, fp), (fn, tp) = cm\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels, yhat, average=\"binary\", zero_division=0)\n",
    "    return {\n",
    "        \"tau\": tau, \"TN\": int(tn), \"FP\": int(fp), \"FN\": int(fn), \"TP\": int(tp),\n",
    "        \"precision\": float(p), \"recall\": float(r), \"f1\": float(f),\n",
    "        \"FPR\": float(fp / (fp + tn + 1e-9)), \"TPR\": float(r),\n",
    "        \"accuracy\": float((tn+tp) / (tn+fp+fn+tp+1e-9))\n",
    "    }\n",
    "\n",
    "for t in [0.0, TAU_EARLY, TAU_BLOCK]:\n",
    "    print(\"—\"*60)\n",
    "    stats = cm_at_tau(scores, y_true, t)\n",
    "    print(f\"τ={stats['tau']:.3f}  \"\n",
    "          f\"TN={stats['TN']} FP={stats['FP']}  FN={stats['FN']} TP={stats['TP']}  \"\n",
    "          f\"Acc={stats['accuracy']:.3f}  P={stats['precision']:.3f}  R={stats['recall']:.3f}  F1={stats['f1']:.3f}  \"\n",
    "          f\"FPR={stats['FPR']:.3f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5) Pearson heatmaps (turn-level & final-turn)\n",
    "# ---------------------------------------------\n",
    "def corr_heatmap(df_num: pd.DataFrame, title: str):\n",
    "    C = df_num.corr(method=\"pearson\")\n",
    "    fig, ax = plt.subplots(figsize=(4.8,4.2))\n",
    "    im = ax.imshow(C.values, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "    ax.set_xticks(range(len(C.columns))); ax.set_xticklabels(C.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(C.index)));   ax.set_yticklabels(C.index)\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            ax.text(j, i, f\"{C.values[i,j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"r\")\n",
    "    ax.set_title(title, fontweight=\"bold\"); fig.tight_layout(); plt.show()\n",
    "\n",
    "# Turn-level\n",
    "corr_heatmap(df_plot[[\"margin\",\"score\",\"label\"]].astype(float), \"Turn-level Pearson\")\n",
    "\n",
    "# Final-turn only (one row per conversation)\n",
    "final_rows = (df_plot.sort_values([\"group_id\",\"turn_local0\"])\n",
    "                      .groupby(\"group_id\", as_index=False).tail(1))\n",
    "corr_heatmap(final_rows[[\"margin\",\"score\",\"label\"]].astype(float), \"Final-turn Pearson\")\n",
    "\n",
    "# Also print r and p for margin ↔ score\n",
    "r_all, p_all = pearsonr(df_plot[\"margin\"].astype(float), df_plot[\"score\"].astype(float))\n",
    "r_fin, p_fin = pearsonr(final_rows[\"margin\"].astype(float), final_rows[\"score\"].astype(float))\n",
    "print(f\"\\nPearson(margin, score) — turn-level: r={r_all:.3f}, p={p_all:.3g}, n={len(df_plot)}\")\n",
    "print(f\"Pearson(margin, score) — final-turn: r={r_fin:.3f}, p={p_fin:.3g}, n={len(final_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead02aff-6c59-464c-9eb5-52e3cf079442",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU_EARLY = 0.20\n",
    "TAU_BLOCK = 0.40\n",
    "\n",
    "def plot_one_jb_conversation(df, gid, tau_early=0.20, tau_block=0.40, use_one_based=False, ylims=None):\n",
    "    g = (df[df[\"group_id\"]==gid]\n",
    "         .sort_values(\"turn_local0\")\n",
    "         .reset_index(drop=True))\n",
    "    if g.empty: return\n",
    "    x = g[\"turn_local1\"].to_numpy() if use_one_based else g[\"turn_local0\"].to_numpy()\n",
    "    y = g[\"margin\"].to_numpy()\n",
    "    s = g[\"score\"].to_numpy()\n",
    "\n",
    "    if ylims is None:\n",
    "        pad = max(y.max()-y.min(), 0.05)\n",
    "        ylo = y.min() - 0.10*pad; yhi = y.max() + 0.15*pad\n",
    "    else:\n",
    "        ylo, yhi = ylims\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 2.8))\n",
    "    ax.axhspan(tau_early, yhi, color=\"#ff7f0e\", alpha=0.10, label=f\"unsafe ≥ {tau_early:.2f}\")\n",
    "    ax.axhspan(tau_block, yhi, color=\"#d62728\", alpha=0.12, label=f\"block ≥ {tau_block:.2f}\")\n",
    "    ax.plot(x, y, lw=2.2, marker=\"o\", markersize=5)\n",
    "\n",
    "    for xi, yi, si in zip(x, y, s):\n",
    "        ax.annotate(f\"{yi:+.2f} (s={si})\", (xi, yi),\n",
    "                    textcoords=\"offset points\", xytext=(0, 8),\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "    ax.axhline(0.0, ls=\"--\", lw=1, color=\"k\", alpha=0.6)\n",
    "    ax.axhline(tau_early, ls=\"--\", lw=1, alpha=0.6)\n",
    "    ax.axhline(tau_block, ls=\"--\", lw=1, alpha=0.6)\n",
    "    ax.set_ylim(ylo, yhi)\n",
    "    ax.set_xlim(x.min()-0.1, x.max()+0.1)\n",
    "    ax.set_xticks(np.arange(x.min(), x.max()+1, 1))\n",
    "    ax.set_xlabel(\"turn index\" + (\" (1-based)\" if use_one_based else \" (0-based)\"))\n",
    "    ax.set_ylabel(\"SVM margin (w·x + b)\")\n",
    "    ax.set_title(f\"{gid[:12]}…   start {y[0]:+.2f} → end {y[-1]:+.2f}\")\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "# same global y-limits across all JB conversations\n",
    "jb_gids = (df_plot[df_plot[\"score\"]==5][\"group_id\"].unique().tolist())\n",
    "if jb_gids:\n",
    "    all_m = np.concatenate([df_plot.loc[df_plot[\"group_id\"]==g, \"margin\"].to_numpy() for g in jb_gids])\n",
    "    pad = max(all_m.max()-all_m.min(), 0.05)\n",
    "    YLIMS = (all_m.min()-0.10*pad, all_m.max()+0.15*pad)\n",
    "else:\n",
    "    YLIMS = None\n",
    "\n",
    "for gid in jb_gids:\n",
    "    plot_one_jb_conversation(df_plot, gid, tau_early=TAU_EARLY, tau_block=TAU_BLOCK,\n",
    "                             use_one_based=False, ylims=YLIMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd61951-a80c-441e-a23e-91770fccbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assumes df_plot has: group_id, turn_idx (or turn_local0), margin, score, label\n",
    "turn_col = \"turn_idx\" if \"turn_idx\" in df_plot.columns else \"turn_local0\"\n",
    "\n",
    "# Final turn per conversation\n",
    "last = (df_plot.sort_values([\"group_id\", turn_col])\n",
    "                .groupby(\"group_id\", as_index=False).tail(1)\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "# Focus on jailbreak conversations (final score = 5)\n",
    "jb = last[last[\"score\"] == 5].copy()\n",
    "N = len(jb)\n",
    "\n",
    "# Pick your thresholds\n",
    "TAU_EARLY = 0.20\n",
    "TAU_BLOCK = 0.40\n",
    "\n",
    "# A1) Final unsafe but margin < 0 (on the \"safe\" side of the plane)\n",
    "a1 = (jb[\"margin\"] < 0).mean()\n",
    "\n",
    "# A2) Final unsafe but margin < TAU_EARLY (below early-warning)\n",
    "a2 = (jb[\"margin\"] < TAU_EARLY).mean()\n",
    "\n",
    "# A3) Final unsafe but margin < TAU_BLOCK (below block threshold)\n",
    "a3 = (jb[\"margin\"] < TAU_BLOCK).mean()\n",
    "\n",
    "# A4) “Downward” trajectories: start > end by a meaningful delta (e.g., ≥ 0.10)\n",
    "deltas = (\n",
    "    df_plot.sort_values([\"group_id\", turn_col])\n",
    "           .groupby(\"group_id\")\n",
    "           .agg(start=(\"margin\",\"first\"), end=(\"margin\",\"last\"))\n",
    "           .reset_index()\n",
    ")\n",
    "deltas = deltas.merge(jb[[\"group_id\"]], on=\"group_id\", how=\"inner\")\n",
    "a4 = (deltas[\"start\"] - deltas[\"end\"] >= 0.10).mean()\n",
    "\n",
    "print(f\"Total jailbreak convos (final score=5): {N}\")\n",
    "print(f\"A1  final margin < 0              : {a1:.2%}\")\n",
    "print(f\"A2  final margin < τ_early {TAU_EARLY:.2f}: {a2:.2%}\")\n",
    "print(f\"A3  final margin < τ_block {TAU_BLOCK:.2f}: {a3:.2%}\")\n",
    "print(f\"A4  start→end decreased ≥ 0.10     : {a4:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a7f0c-fcdf-496e-827f-91a9297fd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- One true place to compute Pearson on exactly-defined subsets ----\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "assert {\"margin\",\"score\",\"group_id\",\"turn_local0\"}.issubset(df_plot.columns)\n",
    "\n",
    "def pearson_from_mask(mask, title):\n",
    "    sub = df_plot.loc[mask, [\"margin\",\"score\"]].astype(float).dropna()\n",
    "    x, y = sub[\"margin\"].to_numpy(), sub[\"score\"].to_numpy()\n",
    "    r, p = pearsonr(x, y)\n",
    "    print(f\"{title}: r={r:.3f} (p={p:.3g}, n={len(sub)})\")\n",
    "    # 2x2 heatmap\n",
    "    C = np.array([[1.0, r],[r, 1.0]])\n",
    "    fig, ax = plt.subplots(figsize=(4.6,4.0))\n",
    "    im = ax.imshow(C, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels([\"margin\",\"score\"], rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels([\"margin\",\"score\"])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, f\"{C[i,j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"r\")\n",
    "    ax.set_title(title, fontweight=\"bold\"); fig.tight_layout(); plt.show()\n",
    "    return r, p\n",
    "\n",
    "# Masks (pick the one you want and use it everywhere)\n",
    "mask_all      = np.isfinite(df_plot[\"margin\"]) & np.isfinite(df_plot[\"score\"])\n",
    "mask_no3      = mask_all & df_plot[\"score\"].isin([1,2,4,5])      # drop 3's\n",
    "mask_final    = mask_no3 & (df_plot.sort_values([\"group_id\",\"turn_local0\"])\n",
    "                            .groupby(\"group_id\").cumcount(ascending=False).eq(0))\n",
    "\n",
    "_ = pearson_from_mask(mask_all,   \"Turn-level Pearson (1/2/3/4/5, ALL turns)\")\n",
    "_ = pearson_from_mask(mask_no3,   \"Turn-level Pearson (no 3’s, ALL turns)\")\n",
    "_ = pearson_from_mask(mask_final, \"Final-turn Pearson (no 3’s, last turn per conv)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ddcae-a5aa-4d1b-834a-d7c67087d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten emb matrix to columns e0..eH-1 for quick inspection (warning: wide CSV)\n",
    "E = np.stack(df_plot[\"emb\"].to_numpy(), axis=0)\n",
    "cols = [f\"e{i}\" for i in range(E.shape[1])]\n",
    "df_save = pd.concat([df_plot.drop(columns=[\"emb\"]).reset_index(drop=True),\n",
    "                     pd.DataFrame(E, columns=cols)], axis=1)\n",
    "out_csv = Path(SAVE_DIR)/\"turn_context_embeddings_with_meta.csv\"\n",
    "df_save.to_csv(out_csv, index=False)\n",
    "print(\"Wrote:\", out_csv, \"shape:\", df_save.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8124af3-c66e-4797-b448-0e27add0e6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nivya-torch)",
   "language": "python",
   "name": "nivya-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
